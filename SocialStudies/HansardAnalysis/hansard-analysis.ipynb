{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&subPath=SocialStudies/HansardAnalysis/hansard-analysis.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Analysis\n",
    "\n",
    "The [Hansard](https://en.wikipedia.org/wiki/Hansard) is a transcript of debates in the Canadian Parliament. It is available from the official [Parliament of Canada website](https://www.parl.ca) as well as other sources such as [Open Parliament](https://openparliament.ca) and [LiPaD: The Linked Parliamentary Data Project](https://www.lipad.ca).\n",
    "\n",
    "We have downloaded the 2020 files from LiPaD, and can load them by selecting the following code cell and clicking the `â–¶Run` button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "\n",
    "hansard = pd.read_csv('https://raw.githubusercontent.com/callysto/data-files/main/SocialStudies/HansardAnalysis/proceedings2020.csv')\n",
    "print(hansard.shape)\n",
    "hansard.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 15 columns and 4945 rows of data.\n",
    "\n",
    "## Who Spoke?\n",
    "\n",
    "Let's have a look at who spoke during these debates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speakers = hansard.drop_duplicates(subset=['speakername'])[['speakername','speakerparty','speakerriding','speakerurl']]\n",
    "speakers = speakers.dropna().reset_index().drop(columns=['index'])\n",
    "print('There were',speakers.shape[0],'speakers from the',speakers['speakerparty'].unique(),'parties.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare that to the list of Members of Parliament from the [43rd Parliament](https://en.wikipedia.org/wiki/43rd_Canadian_Parliament) that started on December 5, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = pd.read_csv('https://www.ourcommons.ca/members/en/search/csv?parliament=43')\n",
    "print('There were',members.shape[0],'Members from the',members['Political Affiliation'].unique(),'parties.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So of the 340 Members of Parliament we had 312 unique speakers, meaning that 28 Members are not recorded as speaking during 2020. Let's see if we can identify who are they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members['Name'] = members['First Name'] +' '+ members['Last Name']\n",
    "silent = []\n",
    "for member in members['Name']:\n",
    "    if member not in speakers['speakername'].values:\n",
    "        silent.append(member)\n",
    "print('That is',len(silent),'Members not recorded as speaking in 2020:')\n",
    "print(silent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course 35 is not equal to 28, but we will leave it to you to compare the list `silent` to the list from `speakers['speakername'].unique()` if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Spoke Most?\n",
    "\n",
    "We can check how many times each speaker is recorded in the Hansard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['speakername'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the length (number of characters) of each of those speeches, and sort them by who said the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['speechlength'] = hansard['speechtext'].str.len()\n",
    "hansard.groupby('speakername').sum().sort_values('speechlength', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot those speech lengths. Feel free to change the `n = 20` variable to see more or fewer Members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "top_length = hansard.groupby('speakername').sum().sort_values('speechlength', ascending=False).head(n)\n",
    "px.bar(top_length, title='Top '+str(n)+' Hansard Speakers by Speech Length').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well, we can check how many times a Member from a political party spoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(hansard['speakerparty'].value_counts(),title='Hansard Speaker Frequency by Party').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_length_party = hansard.groupby('speakerparty').sum().sort_values('speechlength', ascending=False)\n",
    "px.bar(speech_length_party, title='Hansard Speech Length by Party').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We are going to use the Python library [spaCy](https://spacy.io) for natural language processing. On feature of spaCy is the identification of [parts of speech](https://universaldependencies.org/docs/u/pos) (noun, verb, etc.). We will also simplify words to their base forms, which is called \"[lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\" (e.g. the lemma of \"speaking\" is \"speak\").\n",
    "\n",
    "The following code cell creates a new column in our `hansard` DataFrame containing the nouns from the `speechtext` column. It will take about four minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    !pip install spacy --user\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def find_nouns(text):\n",
    "    nouns = []\n",
    "    try:\n",
    "        for token in nlp(text):\n",
    "            if token.pos_ == 'NOUN':\n",
    "                nouns.append(token.lemma_)\n",
    "    except:\n",
    "        pass\n",
    "    return nouns\n",
    "\n",
    "hansard['nouns'] = hansard['speechtext'].apply(find_nouns)\n",
    "print('We now have the columns:', list(hansard.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a bar graph of the most common nouns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = []\n",
    "for row in hansard.itertuples():\n",
    "    for noun in row.nouns:\n",
    "        noun_list.append(noun)\n",
    "nf = pd.DataFrame.from_dict(Counter(noun_list), orient='index')\n",
    "top_nouns = nf.sort_values(0, ascending=False).head(30)\n",
    "px.bar(top_nouns, title='Common Nouns in the House of Commons').update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Parts of Speech\n",
    "\n",
    "Try the following code cell to investigate the frequencies of other [parts of speech](https://universaldependencies.org/docs/u/pos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 'VERB'\n",
    "\n",
    "def find_pos(text):\n",
    "    words = []\n",
    "    try:\n",
    "        for token in nlp(text):\n",
    "            if token.pos_ == pos:\n",
    "                words.append(token.lemma_)\n",
    "    except:\n",
    "        pass\n",
    "    return words\n",
    "column_name = pos.lower()+'s'\n",
    "hansard[column_name] = hansard['speechtext'].apply(find_pos)\n",
    "word_list = []\n",
    "for i, row in hansard.iterrows():\n",
    "    for word in row[column_name]:\n",
    "        word_list.append(word)\n",
    "common_words = pd.DataFrame.from_dict(Counter(word_list), orient='index').sort_values(0, ascending=False).head(30)\n",
    "px.bar(common_words, title='Common '+column_name+' in the House of Commons').update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech for Parties or Individual Speakers\n",
    "\n",
    "Now that we have this NLP dataset, we can see which nouns are most commonly used by each party or speaker.\n",
    "\n",
    "There are some nouns that are quite common, such as `government` and `member`. Since those seem to be more about the business of debate, we can eliminate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pos = 'NOUN'\n",
    "n = 20\n",
    "exclude_words = ['government', 'member', 'people', 'time', 'year', 'legislation', 'bill']\n",
    "\n",
    "for party in hansard['speakerparty'].dropna().unique():\n",
    "    word_list = []\n",
    "    for words in hansard[hansard['speakerparty']==party][pos.lower()+'s']:\n",
    "        for word in words:\n",
    "            if word not in exclude_words:\n",
    "                word_list.append(word)\n",
    "    common_words = pd.DataFrame.from_dict(Counter(word_list), orient='index').sort_values(0, ascending=False).head(n)\n",
    "    title = 'Top '+str(n)+' '+party+' '+pos.lower()+'s'\n",
    "    px.bar(common_words, title=title).update_layout(showlegend=False, height=300).update_xaxes(title={'text':'Word'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = 'Garnett Genuis'\n",
    "pos = 'NOUN'\n",
    "n = 25\n",
    "exclude_words = ['government', 'member', 'people', 'time', 'year', 'legislation', 'bill']\n",
    "\n",
    "word_list = []\n",
    "for words in hansard[hansard['speakername']==speaker][pos.lower()+'s']:\n",
    "    for word in words:\n",
    "        if word not in exclude_words:\n",
    "            word_list.append(word)\n",
    "common_words = pd.DataFrame.from_dict(Counter(word_list), orient='index').sort_values(0, ascending=False).head(n)\n",
    "title = 'Top '+str(n)+' '+pos.lower()+'s'+' spoken by '+speaker\n",
    "px.bar(common_words, title=title).update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Datasets\n",
    "\n",
    "We have been looking at the Hansard from 2020. If you want to explore datasets from other timeframes on [LiPaD](https://www.lipad.ca/full/), you can upload the CSV files to a `new_data` folder here, and run the following code to import them into a DataFrame.\n",
    "\n",
    "```python\n",
    "folder_name = 'new_data'\n",
    "import os\n",
    "import pandas as pd\n",
    "hansard = pd.DataFrame()\n",
    "for root, dirs, files in os.walk(folder_name):\n",
    "    for name in files:\n",
    "        df = pd.read_csv(os.path.join(root, name))\n",
    "        hansard = hansard.append(df)\n",
    "hansard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The Canadian government provides transcripts of debates in the House of Commons, called the [Hansard](https://en.wikipedia.org/wiki/Hansard). In this notebook we imported the Hansard data from 2020 and identified the frequencies of some [parts of speech](https://universaldependencies.org/docs/u/pos) using [natural language processing]([spaCy](https://spacy.io)).\n",
    "\n",
    "Perhaps you can try extension activities such as investigating noun frequency by province or territory, identifying the most common [named entities](https://www.geeksforgeeks.org/python-named-entity-recognition-ner-using-spacy), or creating [word clouds](https://github.com/callysto/curriculum-notebooks/blob/master/EnglishLanguageArts/WordClouds/word-clouds.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1ca6d17674200220921376aaeb3d36cffe15ecab2470a9a5e7a456cdbf61425"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
