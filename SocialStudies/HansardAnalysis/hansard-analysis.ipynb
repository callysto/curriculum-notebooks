{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&subPath=SocialStudies/HansardAnalysis/hansard-analysis.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Analysis\n",
    "\n",
    "The [Hansard](https://en.wikipedia.org/wiki/Hansard) is a transcript of debates in the Canadian Parliament. It is available from the official [Parliament of Canada website](https://www.parl.ca) as well as other sources such as [Open Parliament](https://openparliament.ca) and [LiPaD: The Linked Parliamentary Data Project](https://www.lipad.ca).\n",
    "\n",
    "We have downloaded the 2020 files from LiPaD, and can load them by selecting the following code cell and clicking the `â–¶Run` button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "hansard = pd.read_csv('proceedings2020.csv')\n",
    "print(hansard.shape)\n",
    "hansard.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 15 columns and 4945 rows of data.\n",
    "\n",
    "## Who Spoke?\n",
    "\n",
    "Let's have a look at who spoke during these debates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speakers = hansard.drop_duplicates(subset=['speakername'])[['speakername','speakerparty','speakerriding','speakerurl']]\n",
    "speakers = speakers.dropna().reset_index().drop(columns=['index'])\n",
    "print('There were',speakers.shape[0],'speakers from the',speakers['speakerparty'].unique(),'parties.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare that to the list of Members of Parliament from the [43rd Parliament](https://en.wikipedia.org/wiki/43rd_Canadian_Parliament) that started on December 5, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = pd.read_csv('https://www.ourcommons.ca/members/en/search/csv?parliament=43')\n",
    "print('There were',members.shape[0],'Members from the',members['Political Affiliation'].unique(),'parties.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So of the 340 Members of Parliament we had 312 unique speakers, meaning that 28 Members are not recorded as speaking during 2020. Let's see if we can identify who are they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members['Name'] = members['First Name'] +' '+ members['Last Name']\n",
    "silent = []\n",
    "for member in members['Name']:\n",
    "    if member not in speakers['speakername'].values:\n",
    "        silent.append(member)\n",
    "print('That is',len(silent),'Members not recorded as speaking in 2020:')\n",
    "print(silent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course 35 is not equal to 28, but we will leave it to you to compare the list `silent` to the list from `speakers['speakername'].unique()` if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Spoke Most?\n",
    "\n",
    "We can check how many times each speaker is recorded in the Hansard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['speakername'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the length (number of characters) of each of those speeches, and sort them by who said the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['speechlength'] = hansard['speechtext'].str.len()\n",
    "hansard.groupby('speakername').sum().sort_values('speechlength', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot those speech lengths. Feel free to change the `n = 20` variable to see more or fewer Members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "n = 20\n",
    "top_length = hansard.groupby('speakername').sum().sort_values('speechlength', ascending=False).head(n)\n",
    "px.bar(top_length, title='Top '+str(n)+' Hansard Speakers by Speech Length').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well, we can check how many times a Member from a political party spoke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(hansard['speakerparty'].value_counts(),title='Hansard Speaker Frequency by Party').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_length_party = hansard.groupby('speakerparty').sum().sort_values('speechlength', ascending=False)\n",
    "px.bar(speech_length_party, title='Hansard Speech Length by Party').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We are going to use the Python library [spaCy](https://spacy.io) for natural language processing. It will allow us to remove \"stop words\", which are common words that can be discarded without reducing meaning. We will also simplify words to their base forms, which is called \"[lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\" (e.g. the lemma of \"speaking\" is \"speak\").\n",
    "\n",
    "The following code cell will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except:\n",
    "    !pip install spacy --user\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as sw\n",
    "sw.add(['madam','speaker','hon','member'])\n",
    "\n",
    "def tokenize(text):\n",
    "    speech = []\n",
    "    try:\n",
    "        for token in nlp(text):\n",
    "            if token.is_stop != True and token.is_punct != True:\n",
    "                speech.append(token.lemma_)\n",
    "    except:\n",
    "        pass\n",
    "    return speech\n",
    "\n",
    "hansard['tokenized'] = hansard['speechtext'].apply(tokenize)\n",
    "hansard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = 'Carol Hughes'\n",
    "ch = []\n",
    "for token_list in hansard[hansard['speakername']==speaker]['tokenized']:\n",
    "    ch.extend(token_list)\n",
    "    #print(token_list)\n",
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hansard['speakername']\n",
    "for speaker in hansard['speakername'].unique():\n",
    "    #print(speaker)\n",
    "    df = hansard[hansard['speakername']==speaker]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Datasets\n",
    "\n",
    "If you want to explore datasets from other timeframes on [LiPaD](https://www.lipad.ca/full/), you can upload the CSV files to a `new_data` folder here, and run the following code to import them into a DataFrame.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "hansard = pd.DataFrame()\n",
    "for root, dirs, files in os.walk('new_data'):\n",
    "    for name in files:\n",
    "        df = pd.read_csv(os.path.join(root, name))\n",
    "        hansard = hansard.append(df)\n",
    "hansard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
