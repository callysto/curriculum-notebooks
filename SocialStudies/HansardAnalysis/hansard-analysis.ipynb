{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&subPath=SocialStudies/HansardAnalysis/hansard-analysis.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Analysis\n",
    "\n",
    "The [Hansard](https://en.wikipedia.org/wiki/Hansard) is a transcript of debates in the Canadian Parliament. It is available from the official [Parliament of Canada website](https://www.parl.ca) as well as other sources such as [Open Parliament](https://openparliament.ca) and [LiPaD: The Linked Parliamentary Data Project](https://www.lipad.ca).\n",
    "\n",
    "We have downloaded the 2020 files from LiPaD, and can load them by selecting the following code cell and clicking the `▶Run` button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "except:\n",
    "    !pip install wordcloud\n",
    "    from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "hansard = pd.read_csv('https://raw.githubusercontent.com/callysto/data-files/main/SocialStudies/HansardAnalysis/proceedings2020.csv')\n",
    "print(f'There are {hansard.shape[0]} rows and {hansard.shape[1]} columns of data:')\n",
    "hansard.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Spoke?\n",
    "\n",
    "Let's have a look at who spoke during these debates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "speakers = hansard.drop_duplicates(subset=['speakername'])[['speakername','speakerparty','speakerriding','speakerurl']]\n",
    "speakers = speakers.dropna().reset_index().drop(columns=['index'])\n",
    "print('There were',speakers.shape[0],'speakers from the',speakers['speakerparty'].unique(),'parties.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare that to the list of Members of Parliament from the [43rd Parliament](https://en.wikipedia.org/wiki/43rd_Canadian_Parliament) that started on December 5, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = pd.read_csv('https://www.ourcommons.ca/members/en/search/csv?parliament=43')\n",
    "print('There were',members.shape[0],'Members from the',members['Political Affiliation'].unique(),'parties.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So of the 338 Members of Parliament we had 312 unique speakers, meaning that 26 Members are not recorded as speaking during 2020. Let's see if we can identify who are they were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members['Name'] = members['First Name'] +' '+ members['Last Name']\n",
    "silent = []\n",
    "for member in members['Name']:\n",
    "    if member not in speakers['speakername'].values:\n",
    "        silent.append(member)\n",
    "print('That is',len(silent),'Members not recorded as speaking in 2020:')\n",
    "print(silent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course 36 is not equal to 26, but we will leave it to you to compare the list `silent` to the list from `speakers['speakername'].unique()` if you are interested."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who Spoke Most?\n",
    "\n",
    "We can check how many times each speaker is recorded in the Hansard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_speakers = pd.DataFrame(hansard['speakername'].value_counts())\n",
    "hansard_speakers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate the length (number of characters) of each of those speeches, and sort them by who said the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['speechlength'] = hansard['speechtext'].str.len()\n",
    "hansard.groupby('speakername').sum(numeric_only=True).sort_values('speechlength', ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the number of times *any* MP spoke with a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(hansard_speakers, title='Histogram of Number of Speeches by Member', labels={'value':'Number of times speaking'}).update(layout_showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram is great at showing big-picture trends, but we can also plot those speech lengths on a smaller scale to see the difference. Feel free to change the `n = 20` variable to see more or fewer Members."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "top_length = hansard.groupby('speakername').sum(numeric_only=True).sort_values('speechlength', ascending=False).head(n)\n",
    "px.bar(top_length, title='Top '+str(n)+' Hansard Speakers by Speech Length', labels={'speakername': 'Speaker Name', 'value':'Total words spoken'}).update(layout_showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well, we can check how many times a Member from a political party spoke, as well as the total length of speeches from each party:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(hansard['speakerparty'].value_counts(),title='Hansard Speaker Frequency by Party', labels={'index': 'Party', 'value':'Number of speeches'}).update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_length_party = hansard.groupby('speakerparty').sum(numeric_only=True).sort_values('speechlength', ascending=False)\n",
    "px.bar(speech_length_party, title='Hansard Words Spoken by Party', labels={'speakerparty': 'Party', 'value':'Total words spoken'}).update(layout_showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thinking Proportionally\n",
    "Now, the above plots are useful in finding out which parties spoke the most, but it would be pretty reasonable to expect the parties with the most members to have the longest or most frequent speeches. In the next step, we'll look at the composition of the 43rd Parliament, and normalize the above two plots to the number of members each party has in Parliament:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seats = pd.DataFrame(list(zip(['Liberal', 'Conservative', 'New Democratic Party', 'Bloc Québécois', 'Green Party', 'Independent'],[157, 121, 32, 24, 3, 1])), columns=['Party', 'Seats']).set_index('Party')\n",
    "px.bar(seats, x=seats.index, y='Seats', title='Number of Seats in 43rd Parliament, by Party').update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_norm = pd.DataFrame(hansard['speakerparty'].value_counts()).div(seats['Seats'], axis=0)\n",
    "px.bar(freq_norm,title='Hansard Speaker Frequency by Party (Normalized by Number of Seats)', labels={'index': 'Party', 'value':'Number of speeches (per seat)'}).update(layout_showlegend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_length_party_norm = pd.DataFrame(speech_length_party.div(seats['Seats'], axis=0))\n",
    "px.bar(speech_length_party_norm, title='Hansard Words Spoken by Party (Normalized by Number of Seats)', labels={'index': 'Party', 'value':'Words spoken (per seat)'}).update(layout_showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Topics of Importance\n",
    "We can also look at specific topics that are being addressed the most and vice versa, alongside a particular member of Parliament's topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_topics = pd.DataFrame(hansard.groupby('subtopic')['subtopic'].aggregate('count').reset_index(name='count'))\n",
    "hansard_topics = hansard_topics.sort_values(by=['count']).reset_index()\n",
    "display(hansard_topics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at what the top 10 *most spoken* topics, alongside the top 10 *least spoken* topics at Parliament."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_fig = px.bar(hansard_topics.tail(10), title=\"Top 10 Topics spoken in Parliament\", y=\"subtopic\", x=\"count\", labels={'subtopic': \"Topic\"}, orientation='h', color='count')\n",
    "top_10_fig.update_layout(showlegend=False).update_layout(yaxis_title=None).show()\n",
    "\n",
    "bot_10_fig = px.bar(hansard_topics.head(10), title=\"Bottom 10 Topics spoken in Parliament\", y=\"subtopic\", x=\"count\", labels={'subtopic': \"Topic\"}, orientation='h')\n",
    "bot_10_fig.update_layout(showlegend=False).update_layout(yaxis_title=None).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at both bar charts, are certain topics *not* being addressed as much? Vice-versa, are certain topics you think are being addressed too often?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at which *members of Parliament* speak on topics that you find *important*. In the cell below, input different `subtopic` names in the cell below and see which members of Parliament talk about your particular topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_topics = hansard_topics['subtopic'].unique()\n",
    "print(list_of_topics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above holds all the subtopics spoken in Parliament. The various subtopics can be inputted in the `usr_input` variable in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'Health'\n",
    "\n",
    "members_by_topic = pd.DataFrame(hansard.loc[hansard['subtopic'] == topic]) \n",
    "members_by_topic = members_by_topic.drop_duplicates(subset=['speakername']) \n",
    "members_by_topic = members_by_topic.drop(columns=['basepk', 'hid', 'speechdate', 'pid', 'opid', 'speakerposition', 'subsubtopic', 'speechtext', 'speechtext', 'speakeroldname', 'speakerurl', 'speakerriding']).reset_index(drop=True) \n",
    "if members_by_topic.empty:\n",
    "    print('No matches. Did you make sure to capitalize and space correctly?')\n",
    "else:\n",
    "    display(members_by_topic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate this concept by looking at each party's most important topics using the `speakerparty` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['red', 'orange', 'green', 'blue', 'lightblue', 'lightseagreen']\n",
    "for index, party in enumerate(hansard['speakerparty'].dropna().unique()):\n",
    "    party_topics = pd.DataFrame(hansard.groupby(['subtopic', 'speakerparty'])['subtopic'].aggregate('count').reset_index(name='count'))\n",
    "    party_topics = party_topics.sort_values(by=['count'])\n",
    "    party_topics = party_topics[party_topics['speakerparty'] == party]\n",
    "    fig = px.bar(party_topics.tail(10), title=f\"{party}'s Top 10 Topics\", y='subtopic', x='count', orientation='h')\n",
    "    fig.update_traces(marker_color=colors[index]).update_layout(yaxis_title=None, showlegend=False, height=500).show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1. Which topics stand out between the different parties of Parliament?\n",
    "1. Can you think of reasons why different topics are discussed more in certain parties compared to others?\n",
    "1. What topics would you expect to be to see in certain parties that aren't seen in the top 10?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We are going to use the Python library [spaCy](https://spacy.io) for natural language processing. One feature of spaCy is the identification of [parts of speech](https://universaldependencies.org/docs/u/pos) (noun, verb, etc.). We will also simplify words to their base forms, which is called \"[lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\" (e.g. the lemma of \"speaking\" is \"speak\").\n",
    "\n",
    "The following code cell creates a new column in our `hansard` DataFrame containing the nouns from the `speechtext` column. It will take about four minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    !pip install spacy --user\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "def find_nouns(text):\n",
    "    nouns = []\n",
    "    try:\n",
    "        for token in nlp(text):\n",
    "            if token.pos_ == 'NOUN':\n",
    "                nouns.append(token.lemma_)\n",
    "    except:\n",
    "        pass\n",
    "    return nouns\n",
    "\n",
    "hansard['nouns'] = hansard['speechtext'].apply(find_nouns)\n",
    "print('We now have the columns:', list(hansard.columns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a bar graph of the most common nouns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_list = []\n",
    "for row in hansard.itertuples():\n",
    "    for noun in row.nouns:\n",
    "        noun_list.append(noun)\n",
    "nf = pd.DataFrame.from_dict(Counter(noun_list), orient='index')\n",
    "top_nouns = nf.sort_values(0, ascending=False).head(30)\n",
    "px.bar(top_nouns, title='Common Nouns in the House of Commons', labels={'index':'Noun', 'value':'Count'}).update_layout(showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "1. How do these nouns compare to those you use in your everyday life?\n",
    "1. Can you think of reasons why it would differ?\n",
    "1. What words would you expect to see here that aren't listed in the top 20?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Parts of Speech\n",
    "\n",
    "Try the following code cell to investigate the frequencies of other [parts of speech](https://universaldependencies.org/docs/u/pos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 'PRON'\n",
    "\n",
    "def find_pos(text):\n",
    "    words = []\n",
    "    try:\n",
    "        for token in nlp(text):\n",
    "            if token.pos_ == pos:\n",
    "                words.append(token.lemma_)\n",
    "    except:\n",
    "        pass\n",
    "    return words\n",
    "column_name = pos.lower()+'s'\n",
    "hansard[column_name] = hansard['speechtext'].apply(find_pos)\n",
    "word_list = []\n",
    "for i, row in hansard.iterrows():\n",
    "    for word in row[column_name]:\n",
    "        word_list.append(word)\n",
    "common_words = pd.DataFrame.from_dict(Counter(word_list), orient='index').sort_values(0, ascending=False).head(30)\n",
    "px.bar(common_words, title='Common '+column_name+' in the House of Commons', labels={'index':column_name, 'value':'Count'}).update_layout(showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech for Parties or Individual Speakers\n",
    "\n",
    "Now that we have this NLP dataset, we can see which nouns are most commonly used by each party or speaker.\n",
    "\n",
    "There are some nouns that are quite common, such as `government` and `member`. Since those seem to be more about the business of debate, we can eliminate them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Optional:</b> The below code cell randomly replaces the name of each party in the dataset with a letter, allowing you to determine the party based on their most used words! Another cell after the plots reveals which party is which letter, but if you want to use the party name in the plots you can comment out (place a <tt>#</tt> at the beginning of each line) the below cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly obscure party names\n",
    "import random\n",
    "letters = ['Party A', 'Party B', 'Party C', 'Party D', 'Party E', 'Party F']\n",
    "parties = hansard['speakerparty'].dropna().unique().tolist()\n",
    "random.shuffle(letters)\n",
    "random.shuffle(parties)\n",
    "\n",
    "mapping = {}\n",
    "for key in parties:\n",
    "    for value in letters:\n",
    "        mapping[key] = value\n",
    "        letters.remove(value)\n",
    "        break\n",
    "        \n",
    "hansard['speakerparty'] = hansard['speakerparty'].replace(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 'NOUN'\n",
    "n = 20\n",
    "\n",
    "# These words are so common in how Members speak in the House that they've been excluded\n",
    "exclude_words = ['government', 'member', 'people', 'time', 'year', 'legislation', 'bill']\n",
    "\n",
    "for party in hansard['speakerparty'].dropna().unique():\n",
    "    word_list = []\n",
    "    for words in hansard[hansard['speakerparty']==party][pos.lower()+'s']:\n",
    "        for word in words:\n",
    "            if word not in exclude_words:\n",
    "                word_list.append(word)\n",
    "    common_words = pd.DataFrame.from_dict(Counter(word_list), orient='index').sort_values(0, ascending=False).head(n)\n",
    "    title = 'Top '+str(n)+' '+party+' '+pos.lower()+'s'\n",
    "    px.bar(common_words, title=title, labels={'index': 'Word', 'value': 'Count'}).update_layout(showlegend=False, height=300).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the below cell (remove the `#`) to reveal the party names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By Member\n",
    "\n",
    "Next up, [look up your own Member of Parliament](https://www.ourcommons.ca/members/en) (or any MP) and see what you can learn about their time in the House of Commons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = 'Garnett Genuis'\n",
    "pos = 'NOUN'\n",
    "n = 25\n",
    "exclude_words = ['government', 'member', 'people', 'time', 'year', 'legislation', 'bill']\n",
    "\n",
    "word_list = []\n",
    "for words in hansard[hansard['speakername']==speaker][pos.lower()+'s']:\n",
    "    for word in words:\n",
    "        if word not in exclude_words:\n",
    "            word_list.append(word)\n",
    "common_words = pd.DataFrame.from_dict(Counter(word_list), orient='index').sort_values(0, ascending=False).head(n)\n",
    "title = 'Top '+str(n)+' '+pos.lower()+'s'+' spoken by '+speaker\n",
    "px.bar(common_words, title=title, labels={'index':pos.capitalize(), 'value':'Count'}).update_layout(showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we can also visualize the words as a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "wc = WordCloud(background_color='white')\n",
    "wc.generate_from_frequencies(common_words[0])\n",
    "plt.imshow(wc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Datasets\n",
    "\n",
    "We have been looking at the Hansard from 2020. If you want to explore datasets from other timeframes on [LiPaD](https://www.lipad.ca/full/), you can upload the CSV files to a `new_data` folder here, and run the following code to import them into a DataFrame.\n",
    "\n",
    "```python\n",
    "folder_name = 'new_data'\n",
    "import os\n",
    "import pandas as pd\n",
    "hansard = pd.DataFrame()\n",
    "for root, dirs, files in os.walk(folder_name):\n",
    "    for name in files:\n",
    "        df = pd.read_csv(os.path.join(root, name))\n",
    "        hansard = hansard.append(df)\n",
    "hansard\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The Canadian government provides transcripts of debates in the House of Commons, called the [Hansard](https://en.wikipedia.org/wiki/Hansard). In this notebook we imported the Hansard data from 2020 and identified the frequencies of some [parts of speech](https://universaldependencies.org/docs/u/pos) using [natural language processing]([spaCy](https://spacy.io)).\n",
    "\n",
    "Perhaps you can try extension activities such as investigating noun frequency by province or territory, identifying the most common [named entities](https://www.geeksforgeeks.org/python-named-entity-recognition-ner-using-spacy), or creating [word clouds](https://github.com/callysto/curriculum-notebooks/blob/master/EnglishLanguageArts/WordClouds/word-clouds.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d1ca6d17674200220921376aaeb3d36cffe15ecab2470a9a5e7a456cdbf61425"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
