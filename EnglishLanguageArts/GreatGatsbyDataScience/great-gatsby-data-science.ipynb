{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&subPath=EnglishLanguageArts/GreatGatsbyDataScience/great-gatsby-data-science.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Great Gatsby and Data Science\n",
    "\n",
    "![Great Gatsby Cover Art](https://www.gutenberg.org/cache/epub/64317/pg64317.cover.medium.jpg)\n",
    "\n",
    "Now that [The Great Gatsby](https://en.wikipedia.org/wiki/The_Great_Gatsby) is in the [public domain](https://en.wikipedia.org/wiki/Public_domain), there are no restrictions on how it can be used. We'll get a digital copy of the novel from [Project Gutenberg](https://www.gutenberg.org/ebooks/64317), and then do some data science analysis of the text.\n",
    "\n",
    "To start, click on the code cell below then click the `▶Run` button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_url = 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt'\n",
    "import requests # import the requests code library\n",
    "r = requests.get(book_url) # get the text from the url\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "text = r.text.split('***')[2] # get the part after the header\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'\n",
    "print(text) # print the text of the book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing into Chapters\n",
    "\n",
    "Now that we have the text of the book, let's split it into chapters. We'll use the Python [library](https://en.wikipedia.org/wiki/Library_(computing)) called [pandas](https://pandas.pydata.org) to create a [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that includes the text of each chapter.\n",
    "\n",
    "To determine where a new chapter starts in the text of this book we will look for three blank lines, indicated by the [newline characters](https://en.wikipedia.org/wiki/Newline) `\\r\\n\\r\\n\\r\\n`.\n",
    "\n",
    "`▶Run` the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ch = [] # create an empty list that we will append to\n",
    "for chapter in text.split('\\r\\n\\r\\n\\r\\n'):\n",
    "    chapter = chapter.replace('\\r','').replace('\\n',' ').replace('\\t','') # remove the newline characters\n",
    "    chapter = chapter.replace('  ',' ').replace('  ',' ').strip() # remove extra spaces\n",
    "    chapter = chapter.replace('--','') # remove extra dashes\n",
    "    ch.append(chapter) # append the chapter text to the list\n",
    "book = pd.DataFrame(ch, columns=['Text']) # create a dataframe from the list\n",
    "book = book.drop(book.index[0:3]).reset_index(drop=True) # drop the first three rows because they are not chapters\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 0th \"chapter\" is the poem at the start of the novel, but we can leave that in. However let's remove the Roman numeral from the start of each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1, len(book)):\n",
    "    text_start = book['Text'][x].find(' ')+1\n",
    "    book['Text'][x] = book['Text'][x][text_start:]\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Chapter Length\n",
    "\n",
    "`▶Run` the following cell to create a `Length` column in dataframe. The length will be the number of characters (letters, numbers, spaces, and punctuation) in the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['Length'] = book['Text'].apply(len)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Chapter Lengths\n",
    "\n",
    "Let's create a bar graph and a pie chart of the chapter lengths using the [Plotly Express](https://plotly.com/python/plotly-express/) code library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.bar(book, x=book.index, y='Length', title='Chapter Lengths in <i>The Great Gatsby</i>').update_xaxes(title='Chapter').show()\n",
    "px.pie(book, values='Length', names=book.index, title='Chapter Lengths in <i>The Great Gatsby</i>').update_traces(textinfo='percent+label').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We'll use the [spaCy](https://spacy.io) natural language processing library to process the text and create new columns in our dataframe containing the identified sentences and words. This will allow us to summarize the text and perform some other analysis.\n",
    "\n",
    "It may take a little while after you click `▶Run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import en_core_web_sm\n",
    "    nlp = en_core_web_sm.load() # set up natural language processing\n",
    "except:\n",
    "    !pip install spacy --user\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import en_core_web_sm\n",
    "    nlp = en_core_web_sm.load()\n",
    "\n",
    "def processLanguage(chapter):\n",
    "    processed = nlp(chapter)\n",
    "    sentences = list(processed.sents)\n",
    "    words = [] # create an empty list\n",
    "    for token in processed:\n",
    "        if token.is_alpha: # if the token is a word\n",
    "            words.append(token)\n",
    "    return sentences, words\n",
    "\n",
    "book['Sentences'], book['NLP'] = zip(*book['Text'].apply(processLanguage))\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the Text\n",
    "\n",
    "\n",
    "#### Stop Words\n",
    "\n",
    "To summarize each chapter, we'll start by removing any [stop words](https://en.wikipedia.org/wiki/Stop_word). Stop words are words such as `a`, `it`, `the`, or `and` that don't really add any meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(chapter):\n",
    "    chapter = [word for word in chapter if word.is_stop==False]\n",
    "    return chapter\n",
    "book['SignificantWords'] = book['NLP'].apply(removeStopWords)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Words\n",
    "\n",
    "If we find the frequencies of the signficant words in each each chapter, we will be able to weight each sentence based on how many of those it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def wordFrequencyCounter(chapter):\n",
    "    words = [word.text for word in chapter]\n",
    "    word_frequencies = Counter(words).most_common()\n",
    "    max_frequency = word_frequencies[0][1]\n",
    "    for w in range(len(word_frequencies)):\n",
    "        word_frequencies[w] = (word_frequencies[w][0], word_frequencies[w][1]/max_frequency) # normalize the word counts to values between 0 and 1\n",
    "    return word_frequencies\n",
    "book['SignificantWordFrequencies'] = book['SignificantWords'].apply(wordFrequencyCounter)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Summaries\n",
    "\n",
    "First we will weight the sentences by the frequencies of the significant words, then we'll record the five percent of the sentences that have the highest scores. Again, this might take a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "summaries = []\n",
    "for i in range(len(book)):\n",
    "    sentences = book['Sentences'][i]\n",
    "    number_to_output = int(len(sentences)*0.05) # 5% of the sentences\n",
    "    word_frequencies = book['SignificantWordFrequencies'][i]\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_score = 0\n",
    "        for word in sentence:\n",
    "            for word_frequency in word_frequencies:\n",
    "                if word.pos_ != 'PROPN': # if it is not a proper noun\n",
    "                    if word.text == word_frequency[0]:\n",
    "                        sentence_score += word_frequency[1]\n",
    "        sentence_scores[sentence] = sentence_score\n",
    "    summary = nlargest(number_to_output, sentence_scores, key=sentence_scores.get)\n",
    "    summaries.append(summary)\n",
    "book['Summary'] = summaries\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a basic chapter summary using the syntax `book['Summary'][x]` where `x` is the chapter number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['Summary'][2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech\n",
    "\n",
    "Next we'll identify the [parts of speech](https://spacy.io/api/annotation#pos-tagging) in the text to see how many verbs, adjectives, nouns, and proper nouns there are. We'll also calculate word counts and proportions so we can make some graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['WordCount'] = book['NLP'].apply(len)\n",
    "parts_of_speech = {'Verbs':'VERB', 'Adjectives':'ADJ', 'Nouns':'NOUN', 'ProperNouns':'PROPN'}\n",
    "for part in parts_of_speech:\n",
    "    book[part] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == parts_of_speech[part]])\n",
    "    book[part+'Count'] = book[part].apply(len)\n",
    "    book[part+'%'] = book[part+'Count']/book['WordCount']*100\n",
    "\n",
    "px.line(book, x=book.index, y=['Verbs%', 'Adjectives%', 'Nouns%', 'ProperNouns%'], title='Parts of Speech in <i>The Great Gatsby</i>').update_xaxes(title='Chapter').update_yaxes(title='Percentage')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Proper Nouns\n",
    "\n",
    "The spaCy library does a fairly good job of identifying names, but you may see some false positives (words that aren't actually character names). We can look at the character names and how often they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameCounter(row):\n",
    "    return Counter([token.lemma_.strip() for token in row])\n",
    "book['Names'] = book['Proper Nouns'].apply(nameCounter)\n",
    "name_counter = Counter()\n",
    "for row in book['Names']:\n",
    "    name_counter.update(row)\n",
    "names_df = pd.DataFrame.from_dict(name_counter, orient='index', columns=['Count'])\n",
    "names_df.sort_values('Count', ascending=False, inplace=True)\n",
    "names_df.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove nouns that may categorized incorrectly, or are uncommon proper nouns in the story, let's try only keeping rows with more than `20` occurrences. We can also remove `New`, `York`, `West`, and `East`.\n",
    "\n",
    "Then we'll create a bar chart of the name freqencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_df = names_df[names_df['Count'] > 20]\n",
    "names_df = names_df.drop(['New','York','West','East'])\n",
    "px.bar(names_df, title='Name Freqencies in <i>The Great Gatsby</i>').update_yaxes(title='Frequency').update_xaxes(title='Name').update(layout_showlegend=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name Frequencies over Time\n",
    "\n",
    "Since we have the text divided into chapters, let's visualize the mentions of the top `5` character names throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nameCounter(name):\n",
    "    name_counts = []\n",
    "    for row in book['ProperNouns']:\n",
    "        n = 0\n",
    "        for token in row:\n",
    "            if token.text == name:\n",
    "                n += 1\n",
    "        name_counts.append(n)\n",
    "    return(name_counts)\n",
    "\n",
    "main_characters_list = names_df.head(5).index.to_list()\n",
    "main_characters = pd.DataFrame()\n",
    "for character in main_characters_list:\n",
    "    main_characters[character] = nameCounter(character)\n",
    "    main_characters[character+'Total'] = main_characters[character].cumsum()\n",
    "main_characters['Chapter'] = main_characters.index+1\n",
    "px.line(main_characters, y=main_characters_list, title='Main Character Frequencies in <i>The Great Gatsby</i>').update_yaxes(title='Frequency').update_xaxes(title='Chapter').show()\n",
    "px.line(main_characters, y=[character+'Total' for character in main_characters_list], title='Cumulative Main Character Frequencies in <i>The Great Gatsby</i>').update_yaxes(title='Cumulative Frequency').update_xaxes(title='Chapter').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could, of course, follow a similar process with other parts of speech such as verbs or adjectives.\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is categorizing text based on its tone (negative, neutral, or positive). For this we will use the [vaderSentiment](https://github.com/cjhutto/vaderSentiment) library, then visualize the positive and negative sentiment by chapter.\n",
    "\n",
    "Running the cell below will take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment --user\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "book['Sentiment'] = book['Text'].apply(lambda x: analyzer.polarity_scores(x))\n",
    "book['PositiveSentiment'] = book['Sentiment'].str['pos']\n",
    "book['NegativeSentiment'] = -book['Sentiment'].str['neg']\n",
    "px.bar(book, y=['PositiveSentiment','NegativeSentiment'], title='Sentiment Analysis by Chapter in <i>The Great Gatsby</i>').update_yaxes(title='Sentiment').update_xaxes(title='Chapter').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Hopefully this was an interesting introduction to text statistics such as chapter length, word frequency and word type frequency, and sentiment analysis. You can, of course, use this analyse the text from any other online document in a similar way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1ca6d17674200220921376aaeb3d36cffe15ecab2470a9a5e7a456cdbf61425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
