{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&subPath=EnglishLanguageArts/GreatGatsbyDataScience/great-gatsby-data-science.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Great Gatsby and Data Science\n",
    "\n",
    "![Great Gatsby Cover Art](https://www.gutenberg.org/cache/epub/64317/pg64317.cover.medium.jpg)\n",
    "\n",
    "Now that [The Great Gatsby](https://en.wikipedia.org/wiki/The_Great_Gatsby) is in the [public domain](https://en.wikipedia.org/wiki/Public_domain), there are no restrictions on how it can be used. We'll get a digital copy of the novel from [Project Gutenberg](https://www.gutenberg.org/ebooks/64317), and then do some data science analysis of the text.\n",
    "\n",
    "To start, click on the code cell below then click the `▶Run` button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_url = 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt'\n",
    "import requests # import the requests code library\n",
    "r = requests.get(book_url) # get the text from the url\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "text = r.text.split('***')[2] # get the part after the header\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'\n",
    "print(text) # print the text of the book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing into Chapters\n",
    "\n",
    "Now that we have the text of the book, let's split it into chapters. We'll use the Python [library](https://en.wikipedia.org/wiki/Library_(computing)) called [pandas](https://pandas.pydata.org) to create a [dataframe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html) that includes the text of each chapter.\n",
    "\n",
    "To determine where a new chapter starts in the text of this book we will look for three blank lines, indicated by the [newline characters](https://en.wikipedia.org/wiki/Newline) `\\r\\n\\r\\n\\r\\n`.\n",
    "\n",
    "`▶Run` the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ch = [] # create an empty list that we will append to\n",
    "for chapter in text.split('\\r\\n\\r\\n\\r\\n'):\n",
    "    chapter = chapter.replace('\\r','').replace('\\n',' ').replace('\\t','') # remove the newline characters\n",
    "    chapter = chapter.replace('  ',' ').replace('  ',' ').strip() # remove extra spaces\n",
    "    chapter = chapter.replace('--','') # remove extra dashes\n",
    "    ch.append(chapter) # append the chapter text to the list\n",
    "book = pd.DataFrame(ch, columns=['Text']) # create a dataframe from the list\n",
    "book = book.drop(book.index[0:3]).reset_index(drop=True) # drop the first three rows because they are not chapters\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the 0th \"chapter\" is the poem at the start of the novel, but we can leave that in. However let's remove the Roman numeral from the start of each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1, len(book)):\n",
    "    text_start = book['Text'][x].find(' ')+1\n",
    "    book['Text'][x] = book['Text'][x][text_start:]\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Chapter Length\n",
    "\n",
    "`▶Run` the following cell to create a `Length` column in dataframe. The length will be the number of characters (letters, numbers, spaces, and punctuation) in the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['Length'] = book['Text'].apply(len)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Chapter Lengths\n",
    "\n",
    "Let's create a bar graph and a pie chart of the chapter lengths using the [Plotly Express](https://plotly.com/python/plotly-express/) code library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "px.bar(book, x=book.index, y='Length', title='Chapter Lengths in <i>The Great Gatsby</i>').update_xaxes(title='Chapter').show()\n",
    "px.pie(book, values='Length', names=book.index, title='Chapter Lengths in <i>The Great Gatsby</i>').update_traces(textinfo='percent+label').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "We'll use the [spaCy](https://spacy.io) natural language processing library to process the text and create new columns in our dataframe containing the identified sentences and words. This will allow us to summarize the text and perform some other analysis.\n",
    "\n",
    "It may take a little while after you click `▶Run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following two installation lines if you encounter errors\n",
    "#!pip install spacy --user\n",
    "#!python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load() # set up natural language processing\n",
    "def processLanguage(chapter):\n",
    "    processed = nlp(chapter)\n",
    "    sentences = list(processed.sents)\n",
    "    words = [] # create an empty list\n",
    "    for token in processed:\n",
    "        if token.is_alpha: # if the token is a word\n",
    "            words.append(token)\n",
    "    return sentences, words\n",
    "\n",
    "book['Sentences'], book['NLP'] = zip(*book['Text'].apply(processLanguage))\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing the Text\n",
    "\n",
    "\n",
    "#### Stop Words\n",
    "\n",
    "To summarize each chapter, we'll start by removing any [stop words](https://en.wikipedia.org/wiki/Stop_word). Stop words are words such as `a`, `it`, `the`, or `and` that don't really add any meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(chapter):\n",
    "    chapter = [word for word in chapter if word.is_stop==False]\n",
    "    return chapter\n",
    "book['SignificantWords'] = book['NLP'].apply(removeStopWords)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most Common Words\n",
    "\n",
    "Let's find the signficant words in each each chapter by their relative frequencies. This will allow us to weight each sentence based on how many very significant words it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def wordFrequencyCounter(chapter):\n",
    "    words = [word.text for word in chapter]\n",
    "    word_frequencies = Counter(words).most_common()\n",
    "    max_frequency = word_frequencies[0][1]\n",
    "    for w in range(len(word_frequencies)):\n",
    "        word_frequencies[w] = (word_frequencies[w][0], word_frequencies[w][1]/max_frequency) # normalize the word counts to values between 0 and 1\n",
    "    return word_frequencies\n",
    "book['SignificantWordFrequencies'] = book['SignificantWords'].apply(wordFrequencyCounter)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Summaries\n",
    "\n",
    "First we will weight the sentences by the frequencies of the significant words, then we'll record the five percent of the sentences that have the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "def summarize(sentences):\n",
    "    number_to_output = int(len(sentences)*0.05) # 5% of the sentences\n",
    "    word_frequencies = book['SignificantWordFrequencies'][i]\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_score = 0\n",
    "        for word in sentence:\n",
    "            for word_frequency in word_frequencies:\n",
    "                if word.text == word_frequency[0]:\n",
    "                    sentence_score += word_frequency[1]\n",
    "        sentence_scores[sentence] = sentence_score\n",
    "    summary = nlargest(number_to_output, sentence_scores, key=sentence_scores.get)\n",
    "    return summary\n",
    "#for line in summary:\n",
    "#    print(line)\n",
    "\n",
    "book['Summary'] = book['Sentences'].apply(summarize)\n",
    "book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech\n",
    "\n",
    "Next we'll identify the [parts of speech](https://spacy.io/api/annotation#pos-tagging) in the text to see how many verbs, adjectives, nouns, and proper nouns there are. We'll also calculate word counts and proportions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['Word Count'] = book['NLP'].apply(len)\n",
    "book['Verbs'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'VERB'])\n",
    "book['Verb Count'] = book['Verbs'].apply(len)\n",
    "book['Verb %'] = book['Verb Count']/book['Word Count']*100\n",
    "book['Adjectives'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'ADJ'])\n",
    "book['Adjective Count'] = book['Adjectives'].apply(len)\n",
    "book['Adjective %'] = book['Adjective Count']/book['Word Count']*100\n",
    "book['Nouns'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'NOUN'])\n",
    "book['Noun Count'] = book['Nouns'].apply(len)\n",
    "book['Noun %'] = book['Noun Count']/book['Word Count']*100\n",
    "book['Proper Nouns'] = book['NLP'].apply(lambda x: [token for token in x if token.pos_ == 'PROPN'])\n",
    "book['Proper Noun Count'] = book['Proper Nouns'].apply(len)\n",
    "book['Proper Noun %'] = book['Proper Noun Count']/book['Word Count']*100\n",
    "book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1ca6d17674200220921376aaeb3d36cffe15ecab2470a9a5e7a456cdbf61425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
