{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&subPath=EnglishLanguageArts/PartsOfSpeech/parts-of-speech.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech\n",
    "\n",
    "In this notebook we will look at the usage of various parts of speech in two different public domain books from the site [Project Gutenberg](https://gutenberg.org), Frankenstein and Anne of Green Gables, using a Python natural language processing library called [spaCy](https://spacy.io/).\n",
    "\n",
    "You can run all of the code cells in this notebook by clicking the `▶▶` button in the toolbar above or `Run all` from the `Cell` menu.\n",
    "\n",
    "## Frankenstein\n",
    "\n",
    "by Mary Wollstonecraft (Godwin) Shelley\n",
    "\n",
    "### Parts of Speech Proportions\n",
    "\n",
    "First we'll download the book and create a chart of the relative proportions of the following [parts of speech](https://universaldependencies.org/docs/u/pos):\n",
    "\n",
    "* adjectives\n",
    "* adverbs\n",
    "* verbs\n",
    "* nouns\n",
    "* proper nouns\n",
    "* pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "try:\n",
    "    import en_core_web_sm\n",
    "except:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import en_core_web_sm\n",
    "\n",
    "r = requests.get('http://www.gutenberg.org/files/84/84-0.txt') # get the online book file\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "text = r.text.split('***')[2] # get the part after the header\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'\n",
    "\n",
    "chapter_list = []\n",
    "for chapter in text.split('Chapter'):\n",
    "    if len(chapter)>500: # so that we are getting actual book chapters\n",
    "        chapter = chapter.replace('\\r',' ').replace('\\n',' ').replace('  ', ' ') # delete the 'new line' characters\n",
    "        chapter_list.append(chapter)\n",
    "frankenstein = pd.DataFrame(chapter_list, columns=['Chapter Text'])\n",
    "frankenstein['Length'] = frankenstein['Chapter Text'].apply(len)\n",
    "\n",
    "print('Running natural language processing, please wait...')\n",
    "nlp = en_core_web_sm.load() # set up natural language processing\n",
    "word_types = ['ADJ', 'ADV', 'VERB', 'NOUN', 'PROPN', 'PRON'] # https://universaldependencies.org/docs/u/pos\n",
    "df_list = []\n",
    "for i in range(len(frankenstein)): # iterate through the chapters dataframe\n",
    "    parts_of_speech_list = [] # create an empty list\n",
    "    for token in nlp(frankenstein['Chapter Text'][i]): # loop through each token in the chapter\n",
    "        part_of_speech = token.pos_\n",
    "        if part_of_speech in word_types: # if it is in the list of types we made\n",
    "            parts_of_speech_list.append(part_of_speech)\n",
    "    chapter_length = len(frankenstein['Chapter Text'][i])\n",
    "    word_type_count = {} # create an empty dictionary\n",
    "    for word_type in word_types:\n",
    "        word_type_count.update({word_type:parts_of_speech_list.count(word_type)/chapter_length}) # add to that dictionary\n",
    "    df_list.append(word_type_count)\n",
    "frank_parts_of_speech = pd.DataFrame(df_list)\n",
    "\n",
    "frank_parts_of_speech['Chapter'] = frank_parts_of_speech.index+1 # create a Chapter column\n",
    "frank_parts_of_speech = frank_parts_of_speech.set_index('Chapter') # set the dataframe index to be that new column\n",
    "clear_output()\n",
    "\n",
    "px.line(frank_parts_of_speech, title='Parts of Speech Proportions in Frankenstein by Chapter').update_layout(yaxis_title='Proportion of Total Words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronoun Usage\n",
    "\n",
    "We can also look at the **cumulative** usage of various pronouns throughout the book. This is a concept known as *frequency*, defined as the count of categorized data, but not the data value itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running natural language processing, please wait...')\n",
    "he = [] # create an empty list that we will add to\n",
    "she = []\n",
    "me = []\n",
    "I = []\n",
    "for row_number in range(len(frankenstein)): # loop through the dataframe\n",
    "    he.append(0) # start with zero mentions in this list for this time through the loop\n",
    "    she.append(0)\n",
    "    me.append(0)\n",
    "    I.append(0)\n",
    "    for token in nlp(frankenstein['Chapter Text'][row_number]): # loop through the words in the chapter\n",
    "        if token.lower_ == 'he':\n",
    "            he[row_number] += 1 # increament this variable in the list if the word is \"he\"\n",
    "        if token.lower_ == 'she':\n",
    "            she[row_number] += 1\n",
    "        if token.lower_ == 'me':\n",
    "            me[row_number] += 1\n",
    "        if token.text == 'I':\n",
    "           I[row_number] += 1\n",
    "frankenstein['he'] = he # add the list as a column in the dataframe\n",
    "frankenstein['she'] = she\n",
    "frankenstein['me'] = me\n",
    "frankenstein['I'] = I\n",
    "clear_output()\n",
    "\n",
    "px.line(frankenstein.drop(columns=['Chapter Text','Length']).cumsum(), title='Cumulative Pronouns in Frankenstein').update_layout(yaxis_title='Cumulative Count', xaxis_title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output, the highest cumulative frequency (mathematically defined as the *mode*) of pronouns is the word *I*, at a value count of 2847 by chapter 24. \n",
    "\n",
    "**Can you think of instances where there would be two modes in a given set of data? What circumstances would have to be present in order for this to happen?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell creates a visualization of pronoun usage **by chapter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(frankenstein.drop(columns=['Chapter Text','Length']), title='Pronouns in Frankenstein by Chapter').update_layout(yaxis_title='Count', xaxis_title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anne of Green Gables\n",
    "by Lucy Maud Montgomery\n",
    "\n",
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/files/45/45-0.txt'\n",
    "r = requests.get(url)\n",
    "r.encoding = 'utf-8'\n",
    "text = r.text.split('***')[2]\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "\n",
    "print('Running natural language processing, please wait...')\n",
    "anne_list = []\n",
    "for chapter in text.split('CHAPTER'):\n",
    "    if len(chapter)>500:\n",
    "        chapter = chapter.replace('\\r',' ').replace('\\n',' ').replace('  ', ' ')\n",
    "        anne_list.append({'Chapter Text':chapter, 'Length':len(chapter)})\n",
    "anne = pd.DataFrame(anne_list)\n",
    "\n",
    "word_types = ['ADJ', 'ADV', 'VERB', 'NOUN', 'PRON', 'PROPN'] # https://universaldependencies.org/docs/u/pos\n",
    "df_list = []\n",
    "for i in range(len(anne)):\n",
    "    parts_of_speech_list = []\n",
    "    for token in nlp(anne['Chapter Text'][i]):\n",
    "        part_of_speech = token.pos_\n",
    "        if part_of_speech in word_types:\n",
    "            parts_of_speech_list.append(part_of_speech)\n",
    "    word_type_count = {}\n",
    "    for word_type in word_types:\n",
    "        word_type_count.update({word_type:parts_of_speech_list.count(word_type)/len(anne['Chapter Text'][i])})\n",
    "    df_list.append(word_type_count)\n",
    "anne_parts_of_speech = pd.DataFrame(df_list)\n",
    "anne_parts_of_speech['Chapter'] = anne_parts_of_speech.index+1\n",
    "anne_parts_of_speech = anne_parts_of_speech.set_index('Chapter')\n",
    "clear_output()\n",
    "\n",
    "px.line(anne_parts_of_speech, title='Parts of Speech Proportions in Anne of Green Gables by Chapter').update_layout(yaxis_title='Proportion of Total Words', xaxis_title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronoun Usage\n",
    "\n",
    "**Cumulative** pronoun usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he = []\n",
    "she = []\n",
    "me = []\n",
    "I = []\n",
    "print('Running natural language processing, please wait...')\n",
    "for row_number in range(len(anne)):\n",
    "    he.append(0)\n",
    "    she.append(0)\n",
    "    me.append(0)\n",
    "    I.append(0)\n",
    "    for token in nlp(anne['Chapter Text'][row_number]):\n",
    "        if token.lower_ == 'he':\n",
    "            he[row_number] += 1\n",
    "        if token.lower_ == 'she':\n",
    "            she[row_number] += 1\n",
    "        if token.lower_ == 'me':\n",
    "            me[row_number] += 1\n",
    "        if token.text == 'I':\n",
    "           I[row_number] += 1\n",
    "anne['he'] = he\n",
    "anne['she'] = she\n",
    "anne['me'] = me\n",
    "anne['I'] = I\n",
    "clear_output()\n",
    "\n",
    "px.line(anne.drop(columns=['Chapter Text','Length']).cumsum(), title='Cumulative Pronouns in Anne of Green Gables').update_layout(yaxis_title='Cumulative Count', xaxis_title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronoun usage **by chapter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(anne.drop(columns=['Chapter Text','Length']), title='Pronouns in Anne of Green Gables by Chapter').update_layout(yaxis_title='Count', xaxis_title='Chapter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hopefully this has been an interesting introduction to some natural language processing and text analysis, and inspires you to try some NLP on your own by modifying the Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
