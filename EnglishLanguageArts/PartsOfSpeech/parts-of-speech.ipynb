{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Callysto.ca Banner](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-top.jpg?raw=true)\n",
    "\n",
    "<a href=\"https://hub.callysto.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fcallysto%2Fcurriculum-notebooks&branch=master&subPath=EnglishLanguageArts/PartsOfSpeech/parts-of-speech.ipynb&depth=1\" target=\"_parent\"><img src=\"https://raw.githubusercontent.com/callysto/curriculum-notebooks/master/open-in-callysto-button.svg?sanitize=true\" width=\"123\" height=\"24\" alt=\"Open in Callysto\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech\n",
    "\n",
    "In this notebook we will look at the usage of various parts of speech in two different public domain books from the site [Project Gutenberg](https://gutenberg.org), Frankenstein and Anne of Green Gables, using a Python natural language processing library called [spaCy](https://spacy.io/).\n",
    "\n",
    "You can run all of the code cells in this notebook by clicking the `▶▶` button in the toolbar above or `Run all` from the `Cell` menu.\n",
    "\n",
    "## Frankenstein\n",
    "\n",
    "by Mary Wollstonecraft (Godwin) Shelley\n",
    "\n",
    "### Parts of Speech Proportions\n",
    "\n",
    "First we'll download the book and create a chart of the relative proportions of the following [parts of speech](https://universaldependencies.org/docs/u/pos):\n",
    "\n",
    "* adjectives\n",
    "* adverbs\n",
    "* verbs\n",
    "* nouns\n",
    "* proper nouns\n",
    "* pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('http://www.gutenberg.org/files/84/84-0.txt') # get the online book file\n",
    "r.encoding = 'utf-8' # specify the type of text encoding in the file\n",
    "text = r.text.split('***')[2] # get the part after the header\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"') # replace any 'smart quotes'\n",
    "\n",
    "import pandas as pd\n",
    "frankenstein = pd.DataFrame() # create an empty data frame that we will append to\n",
    "for chapter in text.split('Chapter'):\n",
    "    if len(chapter)>500: # so that we are getting actual book chapters\n",
    "        chapter = chapter.replace('\\r',' ').replace('\\n',' ').replace('  ', ' ') # delete the 'new line' characters\n",
    "        frankenstein = frankenstein.append({'Chapter Text':chapter, 'Length':len(chapter)}, ignore_index=True)\n",
    "\n",
    "try:\n",
    "    import en_core_web_sm\n",
    "except:\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    import en_core_web_sm\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print('Running natural language processing, please wait...')\n",
    "nlp = en_core_web_sm.load() # set up natural language processing\n",
    "word_types = ['ADJ', 'ADV', 'VERB', 'NOUN', 'PROPN', 'PRON'] # https://universaldependencies.org/docs/u/pos\n",
    "parts_of_speech_df = pd.DataFrame(columns=word_types) # create an empty dataframe with column names\n",
    "for i in range(len(frankenstein)): # iterate through the chapters dataframe\n",
    "    parts_of_speech_list = [] # create an empty list\n",
    "    for token in nlp(frankenstein['Chapter Text'][i]): # loop through each token in the chapter\n",
    "        part_of_speech = token.pos_\n",
    "        if part_of_speech in word_types: # if it is in the list of types we made\n",
    "            parts_of_speech_list.append(part_of_speech)\n",
    "    chapter_length = len(frankenstein['Chapter Text'][i])\n",
    "    word_type_count = {} # create an empty dictionary\n",
    "    for word_type in word_types:\n",
    "        word_type_count.update({word_type:parts_of_speech_list.count(word_type)/chapter_length}) # add to that dictionary\n",
    "    parts_of_speech_df = parts_of_speech_df.append(word_type_count, ignore_index=True) # add to dataframe\n",
    "parts_of_speech_df['Chapter'] = parts_of_speech_df.index+1 # create a Chapter column\n",
    "parts_of_speech_df = parts_of_speech_df.set_index('Chapter') # set the dataframe index to be that new column\n",
    "clear_output()\n",
    "pd.options.plotting.backend='plotly' # use the Plotly backend rather than matplotlib\n",
    "parts_of_speech_df.plot.line(title='Parts of Speech Proportions in Frankenstein by Chapter',\n",
    "                             labels=dict(value='Proportion of Total Words'))# create and display the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronoun Usage\n",
    "\n",
    "We can also look at the **cumulative** usage of various pronouns throughout the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running natural language processing, please wait...')\n",
    "he = [] # create an empty list that we will add to\n",
    "she = []\n",
    "me = []\n",
    "I = []\n",
    "for row_number in range(len(frankenstein)): # loop through the dataframe\n",
    "    he.append(0) # start with zero mentions in this list for this time through the loop\n",
    "    she.append(0)\n",
    "    me.append(0)\n",
    "    I.append(0)\n",
    "    for token in nlp(frankenstein['Chapter Text'][row_number]): # loop through the words in the chapter\n",
    "        if token.lower_ == 'he':\n",
    "            he[row_number] += 1 # increament this variable in the list if the word is \"he\"\n",
    "        if token.lower_ == 'she':\n",
    "            she[row_number] += 1\n",
    "        if token.lower_ == 'me':\n",
    "            me[row_number] += 1\n",
    "        if token.text == 'I':\n",
    "           I[row_number] += 1\n",
    "frankenstein['he'] = he # add the list as a column in the dataframe\n",
    "frankenstein['she'] = she\n",
    "frankenstein['me'] = me\n",
    "frankenstein['I'] = I\n",
    "clear_output()\n",
    "frankenstein.drop(columns=['Chapter Text','Length']).cumsum().plot(\n",
    "    title='Cumulative Pronouns in Frankenstein',labels=dict(index='Chapter')) # create and display the visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell creates a visualization of pronoun usage **by chapter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frankenstein.drop(columns=['Chapter Text','Length']).plot(\n",
    "    title='Pronouns in Frankenstein by Chapter',labels=dict(index='Chapter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anne of Green Gables\n",
    "by Lucy Maud Montgomery\n",
    "\n",
    "### Parts of Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/files/45/45-0.txt'\n",
    "r = requests.get(url)\n",
    "r.encoding = 'utf-8'\n",
    "text = r.text.split('***')[2]\n",
    "text = text.replace(\"’\",\"'\").replace(\"“\",'\"').replace(\"”\",'\"')\n",
    "\n",
    "print('Running natural language processing, please wait...')\n",
    "anne = pd.DataFrame()\n",
    "for chapter in text.split('CHAPTER'):\n",
    "    if len(chapter)>500:\n",
    "        chapter = chapter.replace('\\r',' ').replace('\\n',' ').replace('  ', ' ')\n",
    "        anne = anne.append({'Chapter Text':chapter, 'Length':len(chapter)}, ignore_index=True)\n",
    "word_types = ['ADJ', 'ADV', 'VERB', 'NOUN', 'PRON', 'PROPN'] # https://universaldependencies.org/docs/u/pos\n",
    "pos_df = pd.DataFrame(columns=word_types)\n",
    "for i in range(len(anne)):\n",
    "    pos_list = []\n",
    "    for token in nlp(anne['Chapter Text'][i]):\n",
    "        pos = token.pos_\n",
    "        if pos in word_types:\n",
    "            pos_list.append(pos)\n",
    "    word_type_count = {}\n",
    "    for word_type in word_types:\n",
    "        word_type_count.update({word_type:pos_list.count(word_type)/len(anne['Chapter Text'][i])})\n",
    "    pos_df = pos_df.append(word_type_count, ignore_index=True)\n",
    "pos_df['Chapter'] = pos_df.index+1\n",
    "pos_df = pos_df.set_index('Chapter')\n",
    "clear_output()\n",
    "pos_df.plot.line(title='Parts of Speech Proportions in Anne of Green Gables by Chapter',labels=dict(value='Proportion of Total Words'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronoun Usage\n",
    "\n",
    "**Cumulative** pronoun usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he = []\n",
    "she = []\n",
    "me = []\n",
    "I = []\n",
    "print('Running natural language processing, please wait...')\n",
    "for row_number in range(len(anne)):\n",
    "    he.append(0)\n",
    "    she.append(0)\n",
    "    me.append(0)\n",
    "    I.append(0)\n",
    "    for token in nlp(anne['Chapter Text'][row_number]):\n",
    "        if token.lower_ == 'he':\n",
    "            he[row_number] += 1\n",
    "        if token.lower_ == 'she':\n",
    "            she[row_number] += 1\n",
    "        if token.lower_ == 'me':\n",
    "            me[row_number] += 1\n",
    "        if token.text == 'I':\n",
    "           I[row_number] += 1\n",
    "anne['he'] = he\n",
    "anne['she'] = she\n",
    "anne['me'] = me\n",
    "anne['I'] = I\n",
    "clear_output()\n",
    "anne.drop(columns=['Chapter Text','Length']).cumsum().plot(\n",
    "    title='Cumulative Pronouns in Anne of Green Gables',labels=dict(index='Chapter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronoun usage **by chapter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anne.drop(columns=['Chapter Text','Length']).plot(\n",
    "    title='Pronouns in Anne of Green Gables by Chapter',labels=dict(index='Chapter'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Hopefully this has been an interesting introduction to some natural language processing and text analysis, and inspires you to try some NLP on your own by modifying the Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Callysto.ca License](https://github.com/callysto/curriculum-notebooks/blob/master/callysto-notebook-banner-bottom.jpg?raw=true)](https://github.com/callysto/curriculum-notebooks/blob/master/LICENSE.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
